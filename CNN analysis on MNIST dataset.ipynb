{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport time\nimport matplotlib.pyplot as plt\nimport os","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport keras.models","execution_count":2,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import Sequential\nfrom keras.layers import Conv2D, Dense, Add, Flatten, MaxPooling2D, AveragePooling2D, Activation, Input, BatchNormalization, Dropout\nfrom keras.models import Model\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.image import ImageDataGenerator ","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras.optimizers","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model\n!pip install livelossplot","execution_count":5,"outputs":[{"output_type":"stream","text":"Collecting livelossplot\n  Downloading livelossplot-0.5.1-py3-none-any.whl (28 kB)\nRequirement already satisfied: bokeh; python_version >= \"3.6\" in /opt/conda/lib/python3.7/site-packages (from livelossplot) (2.0.1)\nRequirement already satisfied: matplotlib; python_version >= \"3.6\" in /opt/conda/lib/python3.7/site-packages (from livelossplot) (3.2.1)\nRequirement already satisfied: ipython in /opt/conda/lib/python3.7/site-packages (from livelossplot) (7.13.0)\nRequirement already satisfied: PyYAML>=3.10 in /opt/conda/lib/python3.7/site-packages (from bokeh; python_version >= \"3.6\"->livelossplot) (5.3.1)\nRequirement already satisfied: tornado>=5 in /opt/conda/lib/python3.7/site-packages (from bokeh; python_version >= \"3.6\"->livelossplot) (5.0.2)\nRequirement already satisfied: pillow>=4.0 in /opt/conda/lib/python3.7/site-packages (from bokeh; python_version >= \"3.6\"->livelossplot) (5.4.1)\nRequirement already satisfied: packaging>=16.8 in /opt/conda/lib/python3.7/site-packages (from bokeh; python_version >= \"3.6\"->livelossplot) (20.1)\nRequirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.7/site-packages (from bokeh; python_version >= \"3.6\"->livelossplot) (1.18.1)\nRequirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from bokeh; python_version >= \"3.6\"->livelossplot) (3.7.4.1)\nRequirement already satisfied: Jinja2>=2.7 in /opt/conda/lib/python3.7/site-packages (from bokeh; python_version >= \"3.6\"->livelossplot) (2.11.2)\nRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from bokeh; python_version >= \"3.6\"->livelossplot) (2.8.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (1.2.0)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (2.4.7)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (0.10.0)\nRequirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython->livelossplot) (2.6.1)\nRequirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython->livelossplot) (0.1.0)\nRequirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipython->livelossplot) (4.3.3)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython->livelossplot) (4.4.2)\nRequirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython->livelossplot) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython->livelossplot) (3.0.5)\nRequirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.7/site-packages (from ipython->livelossplot) (0.15.2)\nRequirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from ipython->livelossplot) (46.1.3.post20200325)\nRequirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/conda/lib/python3.7/site-packages (from ipython->livelossplot) (4.8.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging>=16.8->bokeh; python_version >= \"3.6\"->livelossplot) (1.14.0)\nRequirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from Jinja2>=2.7->bokeh; python_version >= \"3.6\"->livelossplot) (1.1.1)\nRequirement already satisfied: ipython-genutils in /opt/conda/lib/python3.7/site-packages (from traitlets>=4.2->ipython->livelossplot) (0.2.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->livelossplot) (0.1.9)\nRequirement already satisfied: parso>=0.5.2 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.10->ipython->livelossplot) (0.5.2)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython->livelossplot) (0.6.0)\nInstalling collected packages: livelossplot\nSuccessfully installed livelossplot-0.5.1\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/digit-recognizer/train.csv')\ntrain.describe()\ntrain.columns","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"Index(['label', 'pixel0', 'pixel1', 'pixel2', 'pixel3', 'pixel4', 'pixel5',\n       'pixel6', 'pixel7', 'pixel8',\n       ...\n       'pixel774', 'pixel775', 'pixel776', 'pixel777', 'pixel778', 'pixel779',\n       'pixel780', 'pixel781', 'pixel782', 'pixel783'],\n      dtype='object', length=785)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = train.iloc[:,:].values\nprint(data.shape)\nX = data[:,1:]\nY = data[:,0]\nprint(X.shape, Y.shape)\nXt = tf.convert_to_tensor(X)\nYt=tf.convert_to_tensor(Y)","execution_count":8,"outputs":[{"output_type":"stream","text":"(42000, 785)\n(42000, 784) (42000,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Y)","execution_count":9,"outputs":[{"output_type":"stream","text":"[1 0 1 ... 7 6 9]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(type(Yt))\nprint(Yt[5].numpy())","execution_count":10,"outputs":[{"output_type":"stream","text":"<class 'tensorflow.python.framework.ops.EagerTensor'>\n0\n","name":"stdout"}]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import shutil\nshutil.rmtree('data')\n","execution_count":11,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'data'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-e6da76216cfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0morig_st\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m             \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;31m# lstat()/open()/fstat() trick.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m             \u001b[0morig_st\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data'"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nos.mkdir('data')\nos.mkdir('data/train')\nos.mkdir('data/test')\nfor i in range(10):\n    os.mkdir('data/train/'+str(i))\n    os.mkdir('data/test/'+str(i))\n\ntrain_test_split = 0.2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t1 = time.time()\nfor i in range(X.shape[0]):\n#for i in range(10):\n    seed = np.random.random()\n    img = X[i].reshape(28,28)\n    label = Y[i]\n   # print(label)\n    if seed <= train_test_split:\n        plt.imsave('data/test/'+str(label)+'/img_'+str(i)+'.jpg', img, vmin=0, vmax = 255, cmap='gray' )\n        #plt.figure()\n        #plt.imshow(img,vmin=0, vmax = 255,cmap = 'gray')\n        #plt.show()\n    else:\n        plt.imsave('data/train/'+str(label)+'/img_'+str(i)+'.jpg', img, vmin=0, vmax = 255,cmap='gray' )\n        #plt.figure()\n        #plt.imshow(img,vmin=0, vmax = 255, cmap = 'gray')\n        #plt.show()\nt2 = time.time()\nprint(\"Time required with numpy=\",t2-t1,'\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ***Displaying random images from train/test folders and checking their dimensionality***"},{"metadata":{"trusted":true},"cell_type":"code","source":"path = 'data/train/3/'+str(np.random.choice(os.listdir('data/train/3')))\nprint(path)\nimage = plt.imread(path)\nprint(image.shape)\nplt.imshow(image)\nprint(image[:,:,2]-image[:,:,0])\n'''\nWe conclude that the grayscale images are save as 3D arrays(with each 2D array repeating 3 times) because of np.imsave function, even though they are supposed to be 1D grayscale images\n '''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_from_csv = X[99].reshape(28,28) #99 is chosen by me randomly\nprint(image_from_csv.shape)\nimage_from_csv = np.expand_dims(image_from_csv, 2)\nprint(image_from_csv.shape)\nimage_from_csv = np.repeat(image_from_csv, 3, 2)\nprint(image_from_csv.shape)\nprint(image_from_csv[:,:,2]-image_from_csv[:,:,0])\n'''\nWe have found suitable functions to use in preprocessing function of the Test image generator\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''t1 = time.time()\nfor i in range(Xt.shape[0]):\n    seed = np.random.random()\n    img = tf.reshape(Xt[i],(28,28))\n    label = Yt[i]\n    if seed <= train_test_split:\n        plt.imsave('data/test/'+str(label.numpy())+'/img_'+str(i)+'.jpeg', img, vmin=0, vmax = 255, format='jpg' )\n    else:\n        plt.imsave('data/train/'+str(label.numpy())+'/img_'+str(i)+'.jpeg', img, vmin=0, vmax = 255, format='jpg' )\nt2 = time.time()\nprint(\"Time required with tf=\",t2-t1,'\\n')'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ***Train and Test batch generators***"},{"metadata":{"trusted":true},"cell_type":"code","source":"#hyperparameters\n#print(os.listdir('data/test/0'))\n#print(os.listdir('data/train/0'))\n\nbatch_size = 64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gen = ImageDataGenerator(vertical_flip=True, horizontal_flip=True)\ntrain_batches = train_gen.flow_from_directory('data/train',target_size=(28,28),batch_size = batch_size, shuffle = True)\n\nval_gen = ImageDataGenerator(vertical_flip=True, horizontal_flip=True)\nval_batches = train_gen.flow_from_directory('data/test',target_size=(28,28),batch_size = batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_steps = val_batches.n//batch_size\nsteps_per_epoch = train_batches.n//batch_size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# *** Model 1: Reference Model* **"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Model 1\n\ndef define_model_1 ():  #xin is a batch of inputs from Digit Recogniser Dataset\n    xin = Input((28,28,3),(None,28,28,3))\n    xin_norm = BatchNormalization()(xin)\n    #Layer 1\n    xin_conv_1 = Conv2D(filters = 64, kernel_size = (3,3),strides= 2,activation = 'relu')(xin_norm)\n    bn2 = BatchNormalization()(xin_conv_1)\n    #Layer 2\n    xin_conv_2 = Conv2D(filters = 32, kernel_size = (3,3),strides= 2,padding='same',activation = 'relu')(bn2)\n    bn3 = BatchNormalization()(xin_conv_2)\n    xin_flatten = Flatten()(bn3)\n    xin_dense = Dense(128,activation='relu')(xin_flatten)\n    xout = Dense(10,activation = 'softmax')(xin_dense)\n    model = Model(inputs=xin, outputs=xout)\n    return model\nmodel_1 = define_model_1()\nmodel_1.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor=\"val_acc\",\n    factor=0.1,\n    patience=2,\n    verbose=0,\n    mode=\"auto\",\n    min_delta=0.0001,\n    cooldown=0,\n    min_lr=0\n)\n\nfrom livelossplot import PlotLossesKeras\n\n\nmodel_1.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist1 = model_1.fit(x=train_batches,epochs=10, callbacks=[lr_scheduler,PlotLossesKeras()], \n            validation_data = val_batches,steps_per_epoch = steps_per_epoch, validation_steps = validation_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#training for 5 more epochs to check the upcoming pattern\nhist1_1 = model_1.fit(x=train_batches,epochs=5, callbacks=[lr_scheduler,PlotLossesKeras()], \n            validation_data = val_batches,steps_per_epoch = steps_per_epoch, validation_steps = validation_steps)\n'''\nWe observe that the model overfits the training data as we train for more epochs than sufficient\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_1.get_layer('flatten_1').output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#training for 5 more epochs with lr=0.0001 to check the upcoming pattern\n\nmodel_1.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),loss='categorical_crossentropy',metrics=['acc'])\nhist1_1 = model_1.fit(x=train_batches,epochs=5, callbacks=[lr_scheduler,PlotLossesKeras()], \n            validation_data = val_batches,steps_per_epoch = steps_per_epoch, validation_steps = validation_steps)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#evaluating on test set of Digit Recognizer competition\n\ntest = pd.read_csv('../input/digit-recognizer/test.csv')\ntest.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ***Testing the model on the test.csv file***"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_test(test):    #converts the test.csv DataFrame in pandas to input for the model\n    test = test.iloc[:,:].values\n    print(test.shape)\n    test = np.reshape(test,(test.shape[0],28,28))\n    print(test.shape)\n    test = np.expand_dims(test,3)\n    print(test.shape)\n    test = np.repeat(test, 3, 3)\n    print(test.shape)\n    return test\ntest = preprocess_test(test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#generating random sample from the test sample\nplt.imshow(test[np.random.randint(0,test.shape[0])])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test[3,:,:,0] - test[3,:,:,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = model_1.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(res.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = pd.DataFrame(res) \ntest_pred = pd.DataFrame(test_pred.idxmax(axis = 1)) \ntest_pred.index.name = 'ImageId' \ntest_pred = test_pred.rename(columns = {0: 'Label'}).reset_index() \ntest_pred['ImageId'] = test_pred['ImageId'] + 1\n\ntest_pred.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred.to_csv('res.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ***Model 2: Model 1 but LR of Adam optimizer is set to 0.1 (from 0.001)***"},{"metadata":{"trusted":true},"cell_type":"code","source":"def define_model_2 ():  #xin is a batch of inputs from Digit Recogniser Dataset\n    xin = Input((28,28,3),(None,28,28,3))\n    xin_norm = BatchNormalization()(xin)\n    #Layer 1\n    xin_conv_1 = Conv2D(filters = 64, kernel_size = (3,3),strides= 2,activation = 'relu')(xin_norm)\n    bn2 = BatchNormalization()(xin_conv_1)\n    #Layer 2\n    xin_conv_2 = Conv2D(filters = 32, kernel_size = (3,3),strides= 2,padding='same',activation = 'relu')(bn2)\n    bn3 = BatchNormalization()(xin_conv_2)\n    xin_flatten = Flatten()(bn3)\n    xin_dense = Dense(128,activation='relu')(xin_flatten)\n    xout = Dense(10,activation = 'softmax')(xin_dense)\n    model = Model(inputs=xin, outputs=xout)\n    return model\nmodel_2 = define_model_2()\nmodel_2.summary()\nmodel_2.compile(optimizer=keras.optimizers.Adam(learning_rate=0.1),loss='categorical_crossentropy',metrics=['acc'])\nhist2 = model_2.fit(x=train_batches,epochs=10, callbacks=[lr_scheduler,PlotLossesKeras()], \n            validation_data = val_batches,steps_per_epoch = steps_per_epoch, validation_steps = validation_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#training for 10 more epochs to check if Learning rate scheduler works or not\nmodel_2.fit(x=train_batches,epochs=10, callbacks=[lr_scheduler,PlotLossesKeras()], \n            validation_data = val_batches,steps_per_epoch = steps_per_epoch, validation_steps = validation_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nDue to very high learning rate, the gradient descent actually diverged the weights from \nthe optimal value\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# *Model 3: Model 1 but lr is set to 0.01(from 0.001)*"},{"metadata":{"trusted":true},"cell_type":"code","source":"def define_model_3 ():  #xin is a batch of inputs from Digit Recogniser Dataset\n    xin = Input((28,28,3),(None,28,28,3))\n    xin_norm = BatchNormalization()(xin)\n    #Layer 1\n    xin_conv_1 = Conv2D(filters = 64, kernel_size = (3,3),strides= 2,activation = 'relu')(xin_norm)\n    bn2 = BatchNormalization()(xin_conv_1)\n    #Layer 2\n    xin_conv_2 = Conv2D(filters = 32, kernel_size = (3,3),strides= 2,padding='same',activation = 'relu')(bn2)\n    bn3 = BatchNormalization()(xin_conv_2)\n    xin_flatten = Flatten()(bn3)\n    xin_dense = Dense(128,activation='relu')(xin_flatten)\n    xout = Dense(10,activation = 'softmax')(xin_dense)\n    model = Model(inputs=xin, outputs=xout)\n    return model\nmodel_3 = define_model_3()\nmodel_3.summary()\nmodel_3.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01),loss='categorical_crossentropy',metrics=['acc'])\nhist3 = model_3.fit(x=train_batches,epochs=10, callbacks=[lr_scheduler,PlotLossesKeras()], \n            validation_data = val_batches,steps_per_epoch = steps_per_epoch, validation_steps = validation_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# *Model 4: Model 1 but without using activations in Conv. layers*"},{"metadata":{"trusted":true},"cell_type":"code","source":"def define_model_4 ():  #xin is a batch of inputs from Digit Recogniser Dataset\n    xin = Input((28,28,3),(None,28,28,3))\n    xin_norm = BatchNormalization()(xin)\n    #Layer 1\n    xin_conv_1 = Conv2D(filters = 64, kernel_size = (3,3),strides= 2)(xin_norm)\n    bn2 = BatchNormalization()(xin_conv_1)\n    #Layer 2\n    xin_conv_2 = Conv2D(filters = 32, kernel_size = (3,3),strides= 2,padding='same')(bn2)\n    bn3 = BatchNormalization()(xin_conv_2)\n    xin_flatten = Flatten()(bn3)\n    xin_dense = Dense(128,activation='relu')(xin_flatten)\n    xout = Dense(10,activation = 'softmax')(xin_dense)\n    model = Model(inputs=xin, outputs=xout)\n    return model\nmodel_4 = define_model_4()\nmodel_4.summary()\nmodel_4.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),loss='categorical_crossentropy',metrics=['acc'])\nhist4 = model_4.fit(x=train_batches,epochs=10, callbacks=[lr_scheduler,PlotLossesKeras()], \n            validation_data = val_batches,steps_per_epoch = steps_per_epoch, validation_steps = validation_steps)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# *Model 5: Model 1 but without using activations in all layers except the final Softmax activation*"},{"metadata":{"trusted":true},"cell_type":"code","source":"def define_model_5 ():  #xin is a batch of inputs from Digit Recogniser Dataset\n    xin = Input((28,28,3),(None,28,28,3))\n    xin_norm = BatchNormalization()(xin)\n    #Layer 1\n    xin_conv_1 = Conv2D(filters = 64, kernel_size = (3,3),strides= 2)(xin_norm)\n    bn2 = BatchNormalization()(xin_conv_1)\n    #Layer 2\n    xin_conv_2 = Conv2D(filters = 32, kernel_size = (3,3),strides= 2,padding='same')(bn2)\n    bn3 = BatchNormalization()(xin_conv_2)\n    xin_flatten = Flatten()(bn3)\n    xin_dense = Dense(128)(xin_flatten)\n    xout = Dense(10,activation = 'softmax')(xin_dense)\n    model = Model(inputs=xin, outputs=xout)\n    return model\nmodel_5 = define_model_5()\nmodel_5.summary()\nmodel_5.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),loss='categorical_crossentropy',metrics=['acc'])\nhist5 = model_5.fit(x=train_batches,epochs=10, callbacks=[lr_scheduler,PlotLossesKeras()], \n            validation_data = val_batches,steps_per_epoch = steps_per_epoch, validation_steps = validation_steps)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ***Model 6: Model 1 but one with one extra Dense layer of 64 neurons***"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef define_model_6 ():  #xin is a batch of inputs from Digit Recogniser Dataset\n    xin = Input((28,28,3),(None,28,28,3))\n    xin_norm = BatchNormalization()(xin)\n    #Layer 1\n    xin_conv_1 = Conv2D(filters = 64, kernel_size = (3,3),strides= 2,activation = 'relu')(xin_norm)\n    bn2 = BatchNormalization()(xin_conv_1)\n    #Layer 2\n    xin_conv_2 = Conv2D(filters = 32, kernel_size = (3,3),strides= 2,padding='same',activation = 'relu')(bn2)\n    bn3 = BatchNormalization()(xin_conv_2)\n    xin_flatten = Flatten()(bn3)\n    xin_dense = Dense(128,activation='relu')(xin_flatten)\n    xin_interm = Dense(64,activation='relu')(xin_dense)\n    xout = Dense(10,activation = 'softmax')(xin_interm)\n    model = Model(inputs=xin, outputs=xout)\n    return model\nmodel_6 = define_model_6()\nmodel_6.summary()\nmodel_6.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),loss='categorical_crossentropy',metrics=['acc'])\nhist6 = model_6.fit(x=train_batches,epochs=10, callbacks=[lr_scheduler,PlotLossesKeras()], \n            validation_data = val_batches,steps_per_epoch = steps_per_epoch, validation_steps = validation_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(hist6.history.keys())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ***Model 7: Model 1 but with Dropout***"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ***Model 8: Model 1 but with Focal loss***"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}